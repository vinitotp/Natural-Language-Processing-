# -*- coding: utf-8 -*-
"""NLP Assignment 2 final Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1apfJ_3cLkogye9Ag_CH4AR4Xb9DbrdTO
"""

!python -m spacy download en
!pip install torch
import spacy
spacy.load('en')
import torch
from torchtext import data
from torchtext import datasets
import random
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import torch
from torchtext import data
import spacy
spacy.load('en')
import nltk 
import random 
from nltk.tokenize import word_tokenize
nltk.download('punkt') 
nltk.download('stopwords') 
nltk.download('movie_reviews') 
nltk.download('wordnet')

import csv
import urllib.request as urllib2
import matplotlib.pyplot as plt
import pandas as pd
url = 'https://raw.githubusercontent.com/cacoderquan/Sentiment-Analysis-on-the-Rotten-Tomatoes-movie-review-dataset/master/train.tsv'
response = urllib2.urlopen(url)
dataframe1 = pd.read_csv(response,delimiter='\t',encoding='utf-8')
dataframe1 = dataframe1.sample(frac=1).reset_index(drop=True)
dataframe1.head(10)

X1, X2, Y1, Y2 = train_test_split(dataframe1 ['Phrase'], dataframe1 ['Sentiment'], test_size=0.3, random_state=2003)
documents=[]
X1 = np.array(X1.values.tolist())
Y1 = np.array(Y1.values.tolist())
for i in range(len(X1)):
  documents.append([list(word_tokenize(X1[i])), Y1[i]]) 

X2 = np.array(X2.values.tolist())
Y2 = np.array(Y2.values.tolist())
for i in range(len(X2)):
  documents.append([list(word_tokenize(X2[i])), Y2[i]]) 

documents[0]

from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer 
stem1 = PorterStemmer() 
stem2=LancasterStemmer() 
word_lemma = WordNetLemmatizer() 
stop_english = stopwords.words("english") 
punct="?:!.,;'\"-()"
r_stop = True
useStem = False
useLemma = False
removePuncs = True

for l in range(len(documents)):                   
  label = documents[l][1]                          
  newReview = []                                   
  for w in documents[l][0]:                  
    newWord = w                                    
    if r_stop and (w in stop_english):  
      continue                                    
    if removePuncs and (w in punct):        
      continue                                    
    if useStem:
      newWord = stem2.stem(newWord) 
    if useLemma: 
      newWord = word_lemma.lemmatize(newWord) 
    newReview.append(newWord)                     
  documents[l] = (newReview, label)             
  documents[l] = (' '.join(newReview), label) 

print(documents[0])

dataframe1 = pd.DataFrame(documents, columns=['text', 'sentiment']) 
dataframe1.head()

X1, X2, Y1, Y2 = train_test_split(dataframe1['text'],  dataframe1['sentiment'], test_size=0.3, random_state=2003)

from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer
from keras.utils import to_categorical

vectorizer = TfidfVectorizer(max_features = 2500)
X = vectorizer.fit_transform(dataframe1["text"]) 
Y = dataframe1['sentiment'] 
X1 = vectorizer.transform(X1).toarray()
Y1 = Y1 
X2 = vectorizer.transform(X2).toarray()
Y2 = Y2

Y2

import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv1D, MaxPooling1D
from keras import backend as K

count_classes = 5

X1.shape

Y1 = keras.utils.to_categorical(Y1, count_classes)
Y2 = keras.utils.to_categorical(Y2, count_classes)

Y2

model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3,
                 activation='relu',
                 input_shape=(2500,1)))
model.add(Conv1D(128, kernel_size=3, activation='relu'))
model.add(Conv1D(128, kernel_size=3, activation='relu'))
model.add(Conv1D(128, kernel_size=3, activation='relu'))
model.add(Conv1D(128, kernel_size=3, activation='relu'))
model.add(Conv1D(128, kernel_size=3, activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(MaxPooling1D(pool_size=2))
model.add(MaxPooling1D(pool_size=2))
model.add(MaxPooling1D(pool_size=2))
model.add(MaxPooling1D(pool_size=2))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(rate = 0.25))
model.add(Flatten())
model.add(Dense(count_classes, activation='softmax'))

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adam(),
              metrics=['accuracy',recall_m,precision_m,f1_m])

X1 = X1.reshape(X1.shape[0], X1.shape[1], 1)
X2 = X2.reshape(X2.shape[0], X2.shape[1], 1)

model.fit(X1, Y1,
          batch_size=128,
          epochs=50)
# _, accuracy = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)
score = model.evaluate(X2, Y2, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
loss, accuracy, f1_score, precision, recall = model.evaluate(X2, Y2, verbose=0)

from keras.models import load_model
model.save("model.h5")